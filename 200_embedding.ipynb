{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c387e7ef",
   "metadata": {},
   "source": [
    "#  문장 내의 단어들을 임베딩\n",
    "- keras.layers.Embedding 레이어 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0db507fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "# 샘플 데이터: 간단한 문장들의 모음\n",
    "sentences = [\n",
    "    \"I love machine learning\",\n",
    "    \"I love coding in Python\",\n",
    "    \"Deep learning is fun\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dc90226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 문장을 단어로 나눈 뒤, 단어마다 고유한 인덱스를 부여하는 딕셔너리 생성\n",
    "word_index = {}  # 단어 → 인덱스 매핑 저장용 딕셔너리\n",
    "\n",
    "for sentence in sentences:\n",
    "    for word in sentence.split():         # 문장을 공백 기준으로 단어로 분리\n",
    "        if word not in word_index:        # 아직 등록되지 않은 단어라면\n",
    "            word_index[word] = len(word_index) + 1  # 인덱스를 1부터 시작하여 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98af754a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4], [1, 2, 5, 6, 7], [8, 4, 9, 10]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장들을 단어 인덱스의 시퀀스로 변환\n",
    "sequences = [[word_index[word] for word in sentence.split()] for sentence in sentences]\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51c37c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3,  4,  0],\n",
       "       [ 1,  2,  5,  6,  7],\n",
       "       [ 8,  4,  9, 10,  0]], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장들 중 가장 긴 것의 길이를 구함\n",
    "max_length = max([len(seq) for seq in sequences])\n",
    "\n",
    "# 모든 문장을 가장 긴 문장의 길이로 패딩\n",
    "padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bddcd17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 5, 8)\n",
      "tf.Tensor(\n",
      "[[[-0.00509546  0.04646177 -0.02626221  0.01624841 -0.01500708\n",
      "    0.01860347 -0.0124591  -0.02415131]\n",
      "  [ 0.02978278 -0.04111455 -0.01059643  0.04843226 -0.04749428\n",
      "   -0.02418653  0.02850774  0.02997005]\n",
      "  [ 0.02359163 -0.0089142  -0.01920922  0.01185763  0.02464202\n",
      "   -0.03173651 -0.00041224 -0.01444827]\n",
      "  [ 0.04632372 -0.0354616   0.02728802  0.01915692 -0.01694743\n",
      "   -0.00975879  0.00091573  0.03785068]\n",
      "  [-0.03651553  0.00096764 -0.03580167 -0.00533813 -0.0106707\n",
      "    0.00589209  0.04372511 -0.00064055]]\n",
      "\n",
      " [[-0.00509546  0.04646177 -0.02626221  0.01624841 -0.01500708\n",
      "    0.01860347 -0.0124591  -0.02415131]\n",
      "  [ 0.02978278 -0.04111455 -0.01059643  0.04843226 -0.04749428\n",
      "   -0.02418653  0.02850774  0.02997005]\n",
      "  [ 0.04683686  0.00761987  0.02347115 -0.01368897 -0.03138246\n",
      "    0.01635248 -0.02181448 -0.01266793]\n",
      "  [-0.00163343 -0.02263997  0.00585099  0.00328941 -0.0460024\n",
      "   -0.03340133 -0.02448174 -0.0206647 ]\n",
      "  [ 0.02503947  0.03814009  0.0290934  -0.03859266  0.02948246\n",
      "   -0.03472267  0.02299044 -0.04103234]]\n",
      "\n",
      " [[ 0.03375027 -0.04083429  0.04444989  0.03500061 -0.00420742\n",
      "   -0.01065675 -0.00958491 -0.047435  ]\n",
      "  [ 0.04632372 -0.0354616   0.02728802  0.01915692 -0.01694743\n",
      "   -0.00975879  0.00091573  0.03785068]\n",
      "  [-0.04326652 -0.04774971  0.03075576  0.01864508  0.00163573\n",
      "   -0.04723544  0.03632561  0.01254762]\n",
      "  [-0.01882433 -0.03424244 -0.03111934  0.00156904  0.01873278\n",
      "    0.04290608  0.04262916 -0.0400643 ]\n",
      "  [-0.03651553  0.00096764 -0.03580167 -0.00533813 -0.0106707\n",
      "    0.00589209  0.04372511 -0.00064055]]], shape=(3, 5, 8), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Embedding 레이어 생성\n",
    "embedding_dim = 8\n",
    "embedding_layer = Embedding(input_dim=len(word_index) + 1, output_dim=embedding_dim)\n",
    "\n",
    "# 패딩된 시퀀스를 Embedding 레이어에 통과시켜 임베딩된 결과를 얻음\n",
    "embedded_sequences = embedding_layer(padded_sequences)\n",
    "\n",
    "print(embedded_sequences.shape)\n",
    "print(embedded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "691a1486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Layer Shape : (11, 8)\n",
      "Embedding Layer Weights (Word Embeddings):\n",
      " [[-0.03651553  0.00096764 -0.03580167 -0.00533813 -0.0106707   0.00589209\n",
      "   0.04372511 -0.00064055]\n",
      " [-0.00509546  0.04646177 -0.02626221  0.01624841 -0.01500708  0.01860347\n",
      "  -0.0124591  -0.02415131]\n",
      " [ 0.02978278 -0.04111455 -0.01059643  0.04843226 -0.04749428 -0.02418653\n",
      "   0.02850774  0.02997005]\n",
      " [ 0.02359163 -0.0089142  -0.01920922  0.01185763  0.02464202 -0.03173651\n",
      "  -0.00041224 -0.01444827]\n",
      " [ 0.04632372 -0.0354616   0.02728802  0.01915692 -0.01694743 -0.00975879\n",
      "   0.00091573  0.03785068]\n",
      " [ 0.04683686  0.00761987  0.02347115 -0.01368897 -0.03138246  0.01635248\n",
      "  -0.02181448 -0.01266793]\n",
      " [-0.00163343 -0.02263997  0.00585099  0.00328941 -0.0460024  -0.03340133\n",
      "  -0.02448174 -0.0206647 ]\n",
      " [ 0.02503947  0.03814009  0.0290934  -0.03859266  0.02948246 -0.03472267\n",
      "   0.02299044 -0.04103234]\n",
      " [ 0.03375027 -0.04083429  0.04444989  0.03500061 -0.00420742 -0.01065675\n",
      "  -0.00958491 -0.047435  ]\n",
      " [-0.04326652 -0.04774971  0.03075576  0.01864508  0.00163573 -0.04723544\n",
      "   0.03632561  0.01254762]\n",
      " [-0.01882433 -0.03424244 -0.03111934  0.00156904  0.01873278  0.04290608\n",
      "   0.04262916 -0.0400643 ]]\n",
      "\n",
      "\n",
      "Embedding for 'love':\n",
      " [ 0.02978278 -0.04111455 -0.01059643  0.04843226 -0.04749428 -0.02418653\n",
      "  0.02850774  0.02997005]\n"
     ]
    }
   ],
   "source": [
    "# Embedding 레이어의 가중치 (단어 임베딩 행렬) 출력\n",
    "embeddings = embedding_layer.get_weights()[0]\n",
    "print(\"Embedding Layer Shape :\", embeddings.shape)\n",
    "print(\"Embedding Layer Weights (Word Embeddings):\\n\", embeddings)\n",
    "print()\n",
    "\n",
    "# 예: 'love'라는 단어의 임베딩 벡터를 출력\n",
    "print(\"\\nEmbedding for 'love':\\n\", embeddings[word_index['love']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf45463",
   "metadata": {},
   "source": [
    "0은 보통 패딩을 나타내는 인덱스로 사용됩니다. 결과적으로, Embedding 레이어의 가중치 행렬의 크기는 (고유한 단어 수 + 1, 임베딩 벡터의 차원수)가 되므로, (11, 8)이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0718544",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
